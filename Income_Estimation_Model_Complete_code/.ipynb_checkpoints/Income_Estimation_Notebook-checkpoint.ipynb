{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa91a1d5-5a4d-496d-b2ba-f738259e546b",
   "metadata": {},
   "source": [
    "# <div align=\"center\" style=\"color: #ff5733;\">Income Estimation Regression Model (Catboost) Complete Data</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869184d2-83a5-4e0b-bd3c-30c1e0aed710",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "305c4ace-f20e-4b37-b09c-4ab157af46df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_all_seeds(seed=42):\n",
    "    \"\"\"Set all seeds and environment variables for reproducibility\"\"\"\n",
    "    import os\n",
    "    # Set environment variables before any other imports\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    \n",
    "    # Then set other seeds\n",
    "    import numpy as np\n",
    "    import random\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Force single-thread operations\n",
    "    os.environ['OMP_NUM_THREADS'] = '1'\n",
    "    os.environ['MKL_NUM_THREADS'] = '1'\n",
    "    os.environ['OPENBLAS_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "720773a1-60e8-4089-8c14-332552e2118e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First cell of your notebook\n",
    "set_all_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff95e9d-e647-4b52-a7be-cd159f4c8400",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Beta2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4074a9-afee-4373-b9d0-0db9017e290a",
   "metadata": {},
   "source": [
    "## Declare Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dbf5350-2d76-48ec-b0e1-a2696a1932f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, \n",
    "    mean_squared_error, \n",
    "    r2_score, \n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import catboost as cb\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import shap\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from typing import Union, List\n",
    "from scipy.stats import mstats\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from fuzzywuzzy import fuzz\n",
    "import joblib\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "# Connection to Bigquery\n",
    "client = bigquery.Client(project='prj-prod-dataplatform')\n",
    "\n",
    "import tempfile\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Settings in this Notebook\n",
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd781b8-f1db-431d-b876-ec3801be973e",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "032c5e6d-9fcf-429b-887e-778a97dbf0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "BUCKET_NAME = \"prod-asia-southeast1-tonik-aiml-workspace\"\n",
    "CLOUDPATH = \"Monthly_Income_Estimation/Income_Estimation_Models/Income_Estimation_Notebook/Note_Data_Book\"\n",
    "CLOUDPATH_TARGET = \"Monthly_Income_Estimation/Income_Estimation_Models/Income_Estimation_Notebook/Artifacts\"\n",
    "DATATYPE = \"Step2\"\n",
    "LOCALPATH = \"/home/jupyter/Models/Income_Estimation_Models/Income_Estimation_Model/Income_Estimation_Model_Complete_code/\"\n",
    "MODELNAME = \"Beta2WithOutApp\"\n",
    "VERSIONNAME = \"2_0\"\n",
    "PRODUCT_TYPE = 'SIL_Quick'\n",
    "CURRENT_DATE = datetime.now().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d196ad-3378-4ef5-bbdf-37a2be95e0e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Version Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cc2ee2-891b-4b40-b636-011c334e328c",
   "metadata": {
    "tags": []
   },
   "source": [
    "VERSIONNAME - 2-0\n",
    "\n",
    "With new name of the columns and new table that Bala created\n",
    "\n",
    "Table name -- To be added later\n",
    "\n",
    "Date table created -- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0239ece-d468-46f0-8145-b8b03c680af4",
   "metadata": {
    "tags": []
   },
   "source": [
    "Adding all the functions which might help to move data to google cloud storage or bring it from GCS. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef9edd-d133-412b-8f82-1fec42206112",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726c38b-43d3-4c65-84ab-fc7c9dfa90fc",
   "metadata": {},
   "source": [
    "#### dfdescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17cbccc0-840c-4893-9f84-57e75d6f7f84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dfdescription.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dfdescription.py\n",
    "\n",
    "def dfdescription(df):\n",
    "    print(f\"The shape of the data frame is :\\t {df.shape}\")\n",
    "    print(f\"The data types of columns in dataframe is: \\n{df.dtypes}\")\n",
    "    print(f\"The description of numerical columns is:\\t {df.describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9615d603-fbfe-49ea-9f18-888ef2e72361",
   "metadata": {},
   "source": [
    "## add_column_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8473773-55df-4557-8fcf-faea34940994",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing add_column_prefix.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile add_column_prefix.py\n",
    "\n",
    "def add_column_prefix(df: pd.DataFrame, \n",
    "                      prefix: str, \n",
    "                      columns: Union[str, List[str]] = None):\n",
    "    \"\"\"\n",
    "    Add a prefix to specified columns in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame whose columns need to be renamed\n",
    "    prefix : str\n",
    "        The prefix to be added to selected column names\n",
    "    columns : str or list of str, optional\n",
    "        The specific column(s) to add prefix to. \n",
    "        If None, applies prefix to all columns.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        A new DataFrame with prefixed column names\n",
    "    \n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> data = pd.DataFrame({\n",
    "    ...     'name': ['Alice', 'Bob'], \n",
    "    ...     'age': [25, 30], \n",
    "    ...     'city': ['New York', 'San Francisco']\n",
    "    ... })\n",
    "    >>> \n",
    "    >>> # Add prefix to specific columns\n",
    "    >>> prefixed_data = add_column_prefix(data, 'user_', ['name', 'age'])\n",
    "    >>> print(prefixed_data.columns)\n",
    "    Index(['user_name', 'user_age', 'city'], dtype='object')\n",
    "    \n",
    "    >>> # Add prefix to all columns\n",
    "    >>> all_prefixed = add_column_prefix(data, 'user_')\n",
    "    >>> print(all_prefixed.columns)\n",
    "    Index(['user_name', 'user_age', 'user_city'], dtype='object')\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # If no specific columns are provided, use all columns\n",
    "    if columns is None:\n",
    "        columns = df.columns.tolist()\n",
    "    \n",
    "    # Ensure columns is a list\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "    \n",
    "    # Validate that specified columns exist in the DataFrame\n",
    "    invalid_columns = set(columns) - set(df.columns)\n",
    "    if invalid_columns:\n",
    "        raise ValueError(f\"Columns not found in DataFrame: {invalid_columns}\")\n",
    "    \n",
    "    # Create a dictionary to map selected column names to new column names\n",
    "    rename_dict = {col: f\"{prefix}{col}\" for col in columns}\n",
    "    \n",
    "    # Rename the specified columns\n",
    "    df_copy.rename(columns=rename_dict, inplace=True)\n",
    "    \n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1007b9e7-585f-4d10-b9ce-65f911c75afd",
   "metadata": {},
   "source": [
    "## read_csv_from_gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2f7d773-b754-41f0-adaa-0d8e7bd6e3d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing read_csv_from_gcs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile read_csv_from_gcs.py\n",
    "\n",
    "def read_csv_from_gcs(project_id, bucket_name, file_path):\n",
    "  \"\"\"Reads a CSV file from a GCS bucket into a pandas DataFrame.\n",
    "\n",
    "  Args:\n",
    "    project_id: The Google Cloud project ID.\n",
    "    bucket_name: The name of the GCS bucket.\n",
    "    file_path: The path to the CSV file within the bucket.\n",
    "\n",
    "  Returns:\n",
    "    A pandas DataFrame containing the CSV data.\n",
    "  \"\"\"\n",
    "\n",
    "  storage_client = storage.Client(project=project_id)\n",
    "  bucket = storage_client.bucket(bucket_name)\n",
    "  blob = bucket.blob(file_path)\n",
    "\n",
    "  with blob.open('r') as f:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "  return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4a5a36-6eea-4296-a4a8-1673b7fb7287",
   "metadata": {},
   "source": [
    "## save_df_to_gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c35eb38-1b31-4b52-8f08-5224b8909524",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing read_csv_from_gcs\n"
     ]
    }
   ],
   "source": [
    "%%writefile read_csv_from_gcs\n",
    "\n",
    "def save_df_to_gcs(df, bucket_name, destination_blob_name, file_format='csv'):\n",
    "    \"\"\"Saves a pandas DataFrame to Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame to save.\n",
    "        bucket_name: The name of the GCS bucket.\n",
    "        destination_blob_name: The name of the blob to be created.\n",
    "        file_format: The file format to save the DataFrame in ('csv' or 'parquet').\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a temporary file\n",
    "    if file_format == 'csv':\n",
    "        temp_file = 'temp.csv'\n",
    "        df.to_csv(temp_file, index=False)\n",
    "    elif file_format == 'parquet':\n",
    "        temp_file = 'temp.parquet'\n",
    "        df.to_parquet(temp_file, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid file format. Please choose 'csv' or 'parquet'.\")\n",
    "\n",
    "    # Upload the file to GCS\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(temp_file)\n",
    "\n",
    "    # Remove the temporary file\n",
    "    import os\n",
    "    os.remove(temp_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658e321-f41b-46c5-a1b9-3f9300e44e98",
   "metadata": {},
   "source": [
    "## Upload_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "937db19d-5542-48c1-8888-1f7c557aabeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing upload_blob\n"
     ]
    }
   ],
   "source": [
    "%%writefile upload_blob\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.   \n",
    "\n",
    "\n",
    "    Args:\n",
    "        bucket_name: The name of the bucket.\n",
    "        source_file_name: The path to the file to upload.\n",
    "        destination_blob_name: The name of the blob to be created.\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9ae205-a1b2-4c90-8320-300c35d00983",
   "metadata": {},
   "source": [
    "## upload_model_to_gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7e3fc4a-208b-4304-83ac-b855954686ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing upload_blob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile upload_blob.py\n",
    "\n",
    "# Upload the model to GCS\n",
    "def upload_model_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a model to a GCS bucket.\n",
    "\n",
    "    Args:\n",
    "        bucket_name: The name of the GCS bucket.\n",
    "        source_file_name: The path to the local model file.\n",
    "        destination_blob_name: The name of the blob to be created in GCS.\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b85d2-c238-474e-8537-fcc5c445d01a",
   "metadata": {},
   "source": [
    "## plot_actual_vs_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90a0d0d6-39b6-42de-843b-3c32eddbb582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing plot_actual_vs_predicted.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile plot_actual_vs_predicted.py\n",
    "\n",
    "def plot_actual_vs_predicted(y_true, y_pred, title='Actual vs Predicted Values'):\n",
    "    \"\"\"\n",
    "    Create a scatter plot of actual vs predicted values\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: True target values\n",
    "    - y_pred: Predicted target values\n",
    "    - title: Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c6fa6-ca97-4bf5-b0f4-bb7a925feb7b",
   "metadata": {},
   "source": [
    "## plot_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6764c6c3-2423-49c1-8aa0-2b9de002d9ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing plot_residuals.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile plot_residuals.py\n",
    "\n",
    "def plot_residuals(y_true, y_pred, title='Residual Plot'):\n",
    "    \"\"\"\n",
    "    Create a residual plot to visualize model errors with type conversion\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: True target values\n",
    "    - y_pred: Predicted target values\n",
    "    - title: Plot title\n",
    "    \"\"\"\n",
    "    # Convert to numpy float arrays to ensure type compatibility\n",
    "    y_true_float = np.array(y_true, dtype=float)\n",
    "    y_pred_float = np.array(y_pred, dtype=float)\n",
    "    \n",
    "    residuals = y_true_float - y_pred_float\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_pred_float, residuals, alpha=0.5)\n",
    "    plt.hlines(y=0, xmin=y_pred_float.min(), xmax=y_pred_float.max(), color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623c8d8-2b0f-4c5e-8d25-f27cf223b941",
   "metadata": {},
   "source": [
    "## plot_residuals_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb62c3f2-3a05-4791-b544-8db1970cf6bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing plot_residuals_hist\n"
     ]
    }
   ],
   "source": [
    "%%writefile plot_residuals_hist\n",
    "\n",
    "def plot_residuals_hist(y_true, y_pred, title='Residual Histogram Plot'):\n",
    "    \"\"\"\n",
    "    Create a residual plot to visualize model errors with type conversion\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: True target values\n",
    "    - y_pred: Predicted target values\n",
    "    - title: Plot title\n",
    "    \"\"\"\n",
    "    # Convert to numpy float arrays to ensure type compatibility\n",
    "    y_true_float = np.array(y_true, dtype=float)\n",
    "    y_pred_float = np.array(y_pred, dtype=float)\n",
    "    \n",
    "    residuals = y_true_float - y_pred_float\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(residuals, kde=True)\n",
    "    plt.title('Residual Histogram')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8fd74-fec4-466d-b04c-07f3e49079ae",
   "metadata": {},
   "source": [
    "## plot_lift_chart  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77b42575-c98e-4e5f-87c4-0c2c6ac0109e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing plot_lift_chart.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile plot_lift_chart.py\n",
    "\n",
    "def plot_lift_chart(y_test, y_pred, n_bins=10):\n",
    "    \"\"\"\n",
    "    Plots a lift chart for a regression model.\n",
    "    \n",
    "    Parameters:\n",
    "        y_test (array-like): Actual target values.\n",
    "        y_pred (array-like): Predicted target values.\n",
    "        n_bins (int): Number of bins/quantiles to group the data (default: 10).\n",
    "    \n",
    "    Returns:\n",
    "        None: Displays the lift chart.\n",
    "    \"\"\"\n",
    "    # Combine actual and predicted values into a DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_pred\n",
    "    })\n",
    "    \n",
    "    # Create quantile-based bins\n",
    "    results['Decile'] = pd.qcut(results['Predicted'], q=n_bins, labels=False)\n",
    "\n",
    "    # Group by decile and calculate mean actual and predicted values\n",
    "    lift_chart_data = results.groupby('Decile').agg(\n",
    "        Avg_Predicted=('Predicted', 'mean'),\n",
    "        Avg_Actual=('Actual', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Plot the lift chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(lift_chart_data['Avg_Predicted'], label='Predicted', marker='o')\n",
    "    plt.plot(lift_chart_data['Avg_Actual'], label='Actual', marker='s')\n",
    "    plt.title(\"Lift Chart\")\n",
    "    plt.xlabel(f\"Decile (1-{n_bins})\")\n",
    "    plt.ylabel(\"Average Value\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7007aa12-7e9a-4546-9271-85228f1601dc",
   "metadata": {},
   "source": [
    "## plot_gain_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90aa711f-04fa-4fcc-b458-ef7082791504",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting plot_lift_chart.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile plot_lift_chart.py\n",
    "\n",
    "def plot_gain_chart(y_test, y_pred, n_bins=10):\n",
    "    \"\"\"\n",
    "    Plots a gain chart for a regression model.\n",
    "    \n",
    "    Parameters:\n",
    "        y_test (array-like): Actual target values.\n",
    "        y_pred (array-like): Predicted target values.\n",
    "        n_bins (int): Number of bins/quantiles to group the data (default: 10).\n",
    "    \n",
    "    Returns:\n",
    "        None: Displays the gain chart.\n",
    "    \"\"\"\n",
    "    # Combine actual and predicted values into a DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_pred\n",
    "    })\n",
    "\n",
    "    # Sort by predicted values\n",
    "    results = results.sort_values(by='Predicted', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Calculate cumulative sums for actual and predicted values\n",
    "    results['Cumulative_Actual'] = results['Actual'].cumsum()\n",
    "    results['Cumulative_Predicted'] = results['Predicted'].cumsum()\n",
    "\n",
    "    # Normalize cumulative sums to percentage of total\n",
    "    results['Cumulative_Actual_Percent'] = results['Cumulative_Actual'] / results['Actual'].sum() * 100\n",
    "    results['Cumulative_Predicted_Percent'] = results['Cumulative_Predicted'] / results['Predicted'].sum() * 100\n",
    "    results['Percentage_of_Data'] = np.linspace(1 / len(results), 1, len(results)) * 100\n",
    "\n",
    "    # Plot the gain chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results['Percentage_of_Data'], results['Cumulative_Predicted_Percent'], label='Predicted', marker='o')\n",
    "    plt.plot(results['Percentage_of_Data'], results['Cumulative_Actual_Percent'], label='Actual', marker='s')\n",
    "    plt.title(\"Gain Chart\")\n",
    "    plt.xlabel(\"Percentage of Data\")\n",
    "    plt.ylabel(\"Cumulative Percentage\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b857ec-0b45-4497-96cb-df224809b567",
   "metadata": {},
   "source": [
    "## save_df_to_gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec89e9ac-de30-4baa-960e-a3a913607066",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing save_df_to_gcs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile save_df_to_gcs.py\n",
    "\n",
    "def save_df_to_gcs(df, bucket_name, destination_blob_name, file_format='csv'):\n",
    "    \"\"\"Saves a pandas DataFrame to Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame to save.\n",
    "        bucket_name: The name of the GCS bucket.\n",
    "        destination_blob_name: The name of the blob to be created.\n",
    "        file_format: The file format to save the DataFrame in ('csv' or 'parquet').\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a temporary file\n",
    "    if file_format == 'csv':\n",
    "        temp_file = 'temp.csv'\n",
    "        df.to_csv(temp_file, index=False)\n",
    "    elif file_format == 'parquet':\n",
    "        temp_file = 'temp.parquet'\n",
    "        df.to_parquet(temp_file, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid file format. Please choose 'csv' or 'parquet'.\")\n",
    "\n",
    "    # Upload the file to GCS\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(temp_file)\n",
    "\n",
    "    # Remove the temporary file\n",
    "    import os\n",
    "    os.remove(temp_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c52a31ed-a559-4b7d-b92c-98875d70776f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing check_categorical_columns.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile check_categorical_columns.py\n",
    "\n",
    "## check_categorical_columns\n",
    "\n",
    "def check_categorical_columns(df, categorical_cols):\n",
    "    \"\"\"\n",
    "    Check if any categorical columns contain numerical values or NaNs.\n",
    "    \"\"\"\n",
    "    for col in categorical_cols:\n",
    "        if df[col].dtype != 'object':  # Check if the column is not of type 'object'\n",
    "            print(f\"Column {col} is not of type 'object'. It has type: {df[col].dtype}\")\n",
    "            print(f\"Unique values in {col}: {df[col].unique()}\")\n",
    "        elif df[col].isnull().any():  # Check if the column contains NaN values\n",
    "            print(f\"Column {col} contains NaN values.\")\n",
    "        else:\n",
    "            print(f\"Column {col} seems fine.\")\n",
    "        \n",
    "        # Check for numerical data in categorical columns\n",
    "        numerical_data = df[col][df[col].apply(lambda x: isinstance(x, (int, float)))]\n",
    "        if not numerical_data.empty:\n",
    "            print(f\"Column {col} contains numerical data: {numerical_data.unique()}\")\n",
    "\n",
    "# # List of categorical columns\n",
    "# categorical_cols = ['de_gender', 'de_maritalStatus', 'de_city', 'de_barangay', 'de_province',\n",
    "#                     'de_dependentsCount', 'de_subIndustryDescription', 'de_Education_type',\n",
    "#                     'deviceType', 'osversion_v2', 'brand', 'app_first_app_cat',\n",
    "#                     'app_last_app_cat', 'de_natureofwork_grouped']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82180136-a453-424a-8060-d8178ffc3477",
   "metadata": {},
   "source": [
    "## load_pickle_from_gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aaa3d86b-7708-4dec-901b-7f87a37fd181",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing load_pickle_from_gcs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile load_pickle_from_gcs.py\n",
    "\n",
    "def load_pickle_from_gcs(bucket_name, blob_path):\n",
    "    \"\"\"\n",
    "    Load pickle file from Google Cloud Storage\n",
    "    \n",
    "    Parameters:\n",
    "    bucket_name: Name of the GCS bucket\n",
    "    blob_path: Path to the blob in the bucket\n",
    "    \n",
    "    Returns:\n",
    "    Unpickled data\n",
    "    \"\"\"\n",
    "    from google.cloud import storage\n",
    "    import pickle\n",
    "    import io\n",
    "    \n",
    "    # Initialize GCS client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_path)\n",
    "    \n",
    "    # Download blob content into memory\n",
    "    content = blob.download_as_bytes()\n",
    "    \n",
    "    # Load pickle data from memory\n",
    "    pickle_data = pickle.loads(content)\n",
    "    \n",
    "    return pickle_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2787a5-7bd9-441c-bf54-47e47e18ff0a",
   "metadata": {},
   "source": [
    "# Data preparation and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27478ab-d390-4c20-b48c-623d547bda94",
   "metadata": {},
   "source": [
    "## Query for Other Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f21f3dde-1bc1-4d0f-bc4e-082158af3c53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sq = \"\"\"\n",
    "# with \n",
    "# educate as \n",
    "# (select distinct edu.digitalLoanAccountId, edu.education_id, edu1.description\n",
    "# from `prj-prod-dataplatform.dl_loans_db_raw.tdbk_loan_purpose` edu\n",
    "# inner join (select id, description from dl_loans_db_raw.tdbk_loan_lov_mtb where module = 'Education') edu1 on edu.education_id = edu1.id\n",
    "# ),\n",
    "# educate2 as \n",
    "# (select *, row_number() over(partition by digitalLoanAccountId order by education_id desc) rnk from educate),\n",
    "# educate3 as \n",
    "# (select * from educate2 where rnk = 1),\n",
    "# base as \n",
    "# (select \n",
    "# b.*,\n",
    "# CAST(\n",
    "#             CASE \n",
    "#                 WHEN LOWER(b.osversion_v2) LIKE 'android%' THEN \n",
    "#                     -- Extract just the first number for android\n",
    "#                     CAST(SPLIT(REGEXP_EXTRACT(LOWER(b.osversion_v2), r'android(.+)'), '.')[OFFSET(0)] AS FLOAT64)\n",
    "#                 WHEN LOWER(b.osversion_v2) LIKE 'ios%' THEN\n",
    "#                     -- Extract just the first number for ios\n",
    "#                     CAST(SPLIT(REGEXP_EXTRACT(LOWER(b.osversion_v2), r'ios(.+)'), '.')[OFFSET(0)] AS FLOAT64)\n",
    "#                 ELSE \n",
    "#                     CAST(SPLIT(b.osversion_v2, '.')[OFFSET(0)] AS FLOAT64)\n",
    "#             END AS FLOAT64\n",
    "#         ) as clean_version,\n",
    "#   CASE \n",
    "#     WHEN DATE_TRUNC(b.decision_date, DAY) BETWEEN '2023-07-01' AND '2024-07-31' THEN 'Train'\n",
    "#     WHEN DATE_TRUNC(b.decision_date, DAY) BETWEEN '2024-08-01' AND '2024-08-31' THEN 'Test'\n",
    "#     WHEN DATE_TRUNC(b.decision_date, DAY) BETWEEN '2024-09-01' AND '2024-09-30' THEN 'OOT_SEP_24'\n",
    "#     WHEN DATE_TRUNC(b.decision_date, DAY) BETWEEN '2024-10-01' AND '2024-10-31' THEN 'OOT_OCT_24'\n",
    "#     WHEN DATE_TRUNC(b.decision_date, DAY) BETWEEN '2024-11-01' AND '2024-11-30' THEN 'OOT_NOV_24'\n",
    "#     WHEN DATE_TRUNC(b.decision_date, DAY) BETWEEN '2024-12-01' AND '2024-12-31' THEN 'OOT_DEC_24'\n",
    "# END AS Dataselection,\n",
    "# lmt.loanAccountNumber,\n",
    "# lmt.maritalStatus,\n",
    "# lmt.dependentsCount,\n",
    "# lmt.new_loan_type,\n",
    "# educate3.description Education_type,\n",
    "# from worktable_data_analysis.beta2_loan_details_jan2023_dec2024 b\n",
    "# inner join `risk_credit_mis.loan_master_table` lmt on lmt.digitalLoanAccountid = b.digitalLoanAccountId\n",
    "# left join educate3 on educate3.digitalLoanAccountId = b.digitalLoanAccountId\n",
    "# where b.digitalLoanAccountId is not null\n",
    "# and coalesce(lmt.Max_Ever_DPD, 0) < 10\n",
    "# AND lmt.new_loan_type like'%SIL%'\n",
    "# AND DATE_TRUNC(lmt.termsAndConditionsSubmitDateTime, DAY) >= '2023-07-01'\n",
    "# AND DATE(lmt.thirdDueDate) <= CURRENT_DATE()\n",
    "# AND lmt.flagDisbursement = 1\n",
    "# AND b.user_type in ('2_New Applicant', '1_Repeat Applicant')\n",
    "# )\n",
    "# select \n",
    "# base.cust_id,\n",
    "# base.digitalLoanAccountId,\n",
    "# base.onboarding_datetime,\n",
    "# base.first_name,\n",
    "# base.middle_name,\n",
    "# base.last_name,\n",
    "# base.age,\n",
    "# base.gender,\n",
    "# base.email,\n",
    "# base.onb_mobile_no,\n",
    "# base.onb_city,\n",
    "# base.onb_province,\n",
    "# base.onb_postalcode,\n",
    "# base.onb_barangay,\n",
    "# base.place_of_birth,\n",
    "# base.source_funds_new source_funds,\n",
    "# base.employment_type_new employment_type,\n",
    "# base.nature_of_work_new nature_of_work,\n",
    "# base.industry_description_new  industry_description,\n",
    "# base.onb_document_type,\n",
    "# base.loan_company_name,\n",
    "# base.onb_latitude,\n",
    "# base.onb_longitude,\n",
    "# base.loan_type,\n",
    "# base.user_type,\n",
    "# base.loan_geolocation,\n",
    "# base.loan_docType,\n",
    "# base.loan_docNumber,\n",
    "# base.loan_province,\n",
    "# base.loan_city,\n",
    "# base.loan_postalcode,\n",
    "# base.osversion_v2,\n",
    "# base.decision_date,\n",
    "# base.disbursementDateTime,\n",
    "# date_diff(disbursementDateTime, onboarding_datetime, day) daystoapply,\n",
    "# base.clean_version,\n",
    "# base.Dataselection,\n",
    "# base.loanAccountNumber,\n",
    "# base.Brand,\n",
    "# base.maritalStatus,\n",
    "# base.dependentsCount,\n",
    "# base.Education_type,\n",
    "# case when cast(base.loan_monthly_income as numeric) > 300000 then 300000 else cast(base.loan_monthly_income as numeric) end as loan_monthly_income,\n",
    "# base.loan_monthly_income monthlyIncome\n",
    "# from base \n",
    "# ;\n",
    "# \"\"\"\n",
    "# data = client.query(sq).to_dataframe(progress_bar_type = 'tqdm')\n",
    "# print(f\"The shape of the {MODELNAME}_{DATATYPE}_{VERSIONNAME}_{PRODUCT_TYPE} data is:\\t{data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86245f4b-7f53-4f9b-83f4-d1b07f1105e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['customerId', 'onb_email', 'onb_email_verified_flag', 'first_name',\n",
       "       'middle_name', 'last_name', 'onb_city', 'onb_province',\n",
       "       'onb_postalcode', 'onb_barangay', 'onb_country',\n",
       "       'onb_place_of_birth', 'ln_age', 'ln_gender', 'onb_latitude',\n",
       "       'onb_longitude', 'onb_cnt_ongoing_loans',\n",
       "       'onb_tot_ongoing_loans_emi', 'digitalLoanAccountId',\n",
       "       'loanAccountNumber', 'onb_tsa_onboarding_datetime',\n",
       "       'onb_mobile_no', 'kyc_status', 'kyc_status_upgrade_datetime',\n",
       "       'ln_appln_submit_datetime', 'ln_mobile_no',\n",
       "       'ln_alternate_mobile_no', 'ln_purpose', 'ln_disb_dtime',\n",
       "       'ln_source_funds', 'ln_source_funds_new', 'ln_employment_type',\n",
       "       'ln_employment_type_new', 'ln_nature_of_work',\n",
       "       'ln_nature_of_work_new', 'ln_industry', 'ln_industry_new',\n",
       "       'ln_company_name', 'ln_marital_status', 'ln_dependents_count',\n",
       "       'ln_education_level', 'ln_ref1_type', 'ln_ref2_type',\n",
       "       'ln_address_line', 'ln_province', 'ln_city', 'ln_barangay',\n",
       "       'ln_postal_code', 'ln_geolocation', 'ln_doc_type', 'ln_doc_number',\n",
       "       'ln_user_type', 'ln_loan_type', 'ln_product_type', 'ln_osversion',\n",
       "       'ln_brand', 'ln_self_dec_income', 'ln_salary_scaled_income',\n",
       "       'ln_vas_opted_flag', 'ln_vas_used_flag', 'ln_chosen_principal',\n",
       "       'ln_chosen_tenor', 'ln_mature_fpd30_flag', 'ln_mature_fspd30_flag',\n",
       "       'ln_mature_fstpd30_flag', 'ln_fpd30_flag', 'ln_fspd30_flag',\n",
       "       'ln_fstpd30_flag'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq = \"\"\"select * from  worktable_data_analysis.beta2_loan_details_jan2023_dec2024 limit 10;\"\"\"\n",
    "dummydf = client.query(sq).to_dataframe()\n",
    "dummydf.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dba75d97-5293-4587-b700-8780b185fea0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID 078face0-368b-45bc-8e28-5530efeb449d successfully executed: 100%|\u001b[32m██████████\u001b[0m|\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      "The shape of the Beta2WithOutApp_Step2_1_0_SIL_Quick data is:\t(103926, 42)\n"
     ]
    }
   ],
   "source": [
    "sq = \"\"\"\n",
    "with \n",
    "educate as \n",
    "(select distinct edu.digitalLoanAccountId, edu.education_id, edu1.description\n",
    "from `prj-prod-dataplatform.dl_loans_db_raw.tdbk_loan_purpose` edu\n",
    "inner join (select id, description from dl_loans_db_raw.tdbk_loan_lov_mtb where module = 'Education') edu1 on edu.education_id = edu1.id\n",
    "),\n",
    "educate2 as \n",
    "(select *, row_number() over(partition by digitalLoanAccountId order by education_id desc) rnk from educate),\n",
    "educate3 as \n",
    "(select * from educate2 where rnk = 1),\n",
    "base as \n",
    "(select \n",
    "b.customerId, b.onb_email, b.onb_email_verified_flag,\n",
    "       b.onb_place_of_birth, b.age, b.onb_latitude, b.onb_longitude,\n",
    "       b.onb_cnt_ongoing_loans, b.onb_tot_ongoing_loans_emi,\n",
    "       b.digitalLoanAccountId, b.loanAccountNumber, b.onboarding_datetime,\n",
    "       b.onb_mobile_no, b.loan_mobile_no, b.loan_alternate_mobile_no,\n",
    "       b.loan_purpose, b.loan_disbursementDateTime, b.loan_source_funds,\n",
    "       b.loan_source_funds_new, b.loan_employment_type,\n",
    "       b.loan_employment_type_new, b.loan_nature_of_work,\n",
    "       b.loan_nature_of_work_new, b.loan_industry_description,\n",
    "       b.loan_industry_description_new, b.loan_companyName,\n",
    "       b.loan_marital_status, b.loan_dependents_count,\n",
    "       b.loan_education_level, b.loan_ref_type1, b.loan_ref_type2,\n",
    "       b.loan_addressline, b.loan_province, b.loan_city, b.loan_barangay,\n",
    "       b.loan_postalcode, b.loan_geolocation, b.loan_docType,\n",
    "       b.loan_docNumber, b.loan_type, b.loan_product_type,\n",
    "       b.loan_osversion_v2, b.loan_brand, b.loan_self_dec_income,\n",
    "       b.loan_salary_scaled_income, b.loan_vas_opted_flag\n",
    ",\n",
    "CAST(\n",
    "            CASE \n",
    "                WHEN LOWER(b.loan_osversion_v2) LIKE 'android%' THEN \n",
    "                    -- Extract just the first number for android\n",
    "                    CAST(SPLIT(REGEXP_EXTRACT(LOWER(b.loan_osversion_v2), r'android(.+)'), '.')[OFFSET(0)] AS FLOAT64)\n",
    "                WHEN LOWER(b.loan_osversion_v2) LIKE 'ios%' THEN\n",
    "                    -- Extract just the first number for ios\n",
    "                    CAST(SPLIT(REGEXP_EXTRACT(LOWER(b.loan_osversion_v2), r'ios(.+)'), '.')[OFFSET(0)] AS FLOAT64)\n",
    "                ELSE \n",
    "                    CAST(SPLIT(b.loan_osversion_v2, '.')[OFFSET(0)] AS FLOAT64)\n",
    "            END AS FLOAT64\n",
    "        ) as clean_version,\n",
    "  CASE \n",
    "    WHEN DATE_TRUNC(lmt.decision_date, DAY) BETWEEN '2023-07-01' AND '2024-07-31' THEN 'Train'\n",
    "    WHEN DATE_TRUNC(lmt.decision_date, DAY) BETWEEN '2024-08-01' AND '2024-08-31' THEN 'Test'\n",
    "    WHEN DATE_TRUNC(lmt.decision_date, DAY) BETWEEN '2024-09-01' AND '2024-09-30' THEN 'OOT_SEP_24'\n",
    "    WHEN DATE_TRUNC(lmt.decision_date, DAY) BETWEEN '2024-10-01' AND '2024-10-31' THEN 'OOT_OCT_24'\n",
    "    WHEN DATE_TRUNC(lmt.decision_date, DAY) BETWEEN '2024-11-01' AND '2024-11-30' THEN 'OOT_NOV_24'\n",
    "    WHEN DATE_TRUNC(lmt.decision_date, DAY) BETWEEN '2024-12-01' AND '2024-12-31' THEN 'OOT_DEC_24'\n",
    "END AS Dataselection,\n",
    "lmt.disbursementDateTime,\n",
    "lmt.new_loan_type,\n",
    "lmt.Gender,\n",
    "from worktable_data_analysis.beta2_loan_details_jan2023_dec2024 b\n",
    "inner join `risk_credit_mis.loan_master_table` lmt on lmt.digitalLoanAccountid = b.digitalLoanAccountId\n",
    "left join educate3 on educate3.digitalLoanAccountId = b.digitalLoanAccountId\n",
    "where b.digitalLoanAccountId is not null\n",
    "and coalesce(lmt.Max_Ever_DPD, 0) < 10\n",
    "AND (upper(lmt.new_loan_type) like '%SIL%' or upper(lmt.new_loan_type) like '%QUICK%')\n",
    "AND DATE_TRUNC(lmt.termsAndConditionsSubmitDateTime, DAY) >= '2023-07-01'\n",
    "AND DATE(lmt.thirdDueDate) <= CURRENT_DATE()\n",
    "AND lmt.flagDisbursement = 1\n",
    "-- AND b.user_type in ('2_New Applicant', '1_Repeat Applicant')\n",
    ")\n",
    "select \n",
    "base.customerId cust_id,\n",
    "base.digitalLoanAccountId,\n",
    "base.loanAccountNumber,\n",
    "base.onboarding_datetime,\n",
    "base.age,\n",
    "base.Gender,\n",
    "base.onb_email email,\n",
    "base.onb_mobile_no onb_mobile_no,\n",
    "base.loan_mobile_no,\n",
    "Case when coalesce(base.onb_mobile_no, '0') = coalesce(base.loan_mobile_no, '0') then 0 else 1 end onb_mobile_Not_match_loan_mobile,\n",
    "case when loan_alternate_mobile_no is null then 1 else 0 end flag_alternate_mobile_provided,\n",
    "base.loan_purpose,\n",
    "base.loan_source_funds_new source_funds,\n",
    "base.loan_employment_type_new employment_type,\n",
    "base.loan_nature_of_work_new nature_of_work,\n",
    "base.loan_industry_description_new industry_description,\n",
    "base.loan_companyName loan_company_name,\n",
    "base.loan_marital_status maritalStatus,\n",
    "base.loan_dependents_count dependentsCount,\n",
    "base.loan_education_level,\n",
    "base.loan_ref_type1,\n",
    "base.loan_ref_type2,\n",
    "base.loan_province, \n",
    "base.loan_city,\n",
    "base.loan_barangay,\n",
    "base.loan_postalcode,\n",
    "base.loan_geolocation,\n",
    "base.loan_product_type,\n",
    "base.loan_osversion_v2 osversion_v2,\n",
    "base.clean_version,\n",
    "base.loan_brand,\n",
    "base.loan_self_dec_income monthlyIncome,\n",
    "case when cast(base.loan_self_dec_income as numeric) > 300000 then 300000 else cast(base.loan_self_dec_income as numeric) end as loan_monthly_income,\n",
    "base.loan_vas_opted_flag,\n",
    "base.Dataselection,\n",
    "base.onb_place_of_birth place_of_birth,\n",
    "base.onb_latitude,\n",
    "base.onb_longitude,\n",
    "base.loan_type,\n",
    "base.loan_docType,\n",
    "base.loan_docNumber,\n",
    "date_diff(disbursementDateTime, onboarding_datetime, day) daystoapply,\n",
    "from base \n",
    ";\n",
    "\"\"\"\n",
    "data = client.query(sq).to_dataframe(progress_bar_type = 'tqdm')\n",
    "print(f\"The shape of the {MODELNAME}_{DATATYPE}_{VERSIONNAME}_{PRODUCT_TYPE} data is:\\t{data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb81dd36-76de-4e5f-af42-3c2f0b141282",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataframe after selecting only digitalLoanAccountId and loan_monthly_income is:\t (103926, 3)\n"
     ]
    }
   ],
   "source": [
    "lmi_df = data[['digitalLoanAccountId', 'loan_monthly_income', 'monthlyIncome']].copy()\n",
    "print(f\"The shape of the dataframe after selecting only digitalLoanAccountId and loan_monthly_income is:\\t {lmi_df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "incomeestimation_env",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "IncomeEstimation_env",
   "language": "python",
   "name": "incomeestimation_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
